{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U9Sa2KO4javZ"
      },
      "outputs": [],
      "source": [
        "#Import the libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the required stopwords and tokenizers\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbdEsWNrkC43",
        "outputId": "dc166803-a0ec-4bf0-c0bc-df096f4abd57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sample text to summarize\n",
        "text= \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "The fox is running fast to catch its prey. Suddenly, it sees the prey and jumps over it.\n",
        "The prey escapes and the fox continues to run.\n",
        "The dog wakes up and barks at the fox.\n",
        "The fox runs away and the dog goes back to sleep.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "mQEaITxQkO4y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "hOsdEQXZlPF_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Ensure required data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stemmer and stop words\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "words = []\n",
        "\n",
        "# Remove stopwords and stem the words in each sentence\n",
        "for sentence in sentences:\n",
        "    for word in nltk.word_tokenize(sentence.lower()):  # Corrected tokenize method\n",
        "        if word not in stop_words and word.isalpha():  # Only consider alphabetic words\n",
        "            words.append(stemmer.stem(word))\n",
        "\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWhV_RYBlYzO",
        "outputId": "a46db7c5-0bbb-4e7c-91cd-6ce19b3cbabb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'fox', 'run', 'fast', 'catch', 'prey', 'suddenli', 'see', 'prey', 'jump', 'prey', 'escap', 'fox', 'continu', 'run', 'dog', 'wake', 'bark', 'fox', 'fox', 'run', 'away', 'dog', 'goe', 'back', 'sleep']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the count of frequency of each word in the text\n",
        "freq_dist = nltk.FreqDist(words)"
      ],
      "metadata": {
        "id": "uWB_RMzpnGNC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the top 10 most frequent words in the text\n",
        "top_words = [word[0] for word in freq_dist.most_common(10)]"
      ],
      "metadata": {
        "id": "uyQXjkzvpOQS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQz-N0yypvCb",
        "outputId": "c52d56ab-447c-4372-c7ca-e1609c32975d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fox',\n",
              " 'dog',\n",
              " 'run',\n",
              " 'prey',\n",
              " 'jump',\n",
              " 'quick',\n",
              " 'brown',\n",
              " 'lazi',\n",
              " 'fast',\n",
              " 'catch']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a summary by selecting the 3 sentences with the most frequent words\n",
        "summary=[]\n",
        "for sentence in sentences:\n",
        "  sentence_words=nltk.word_tokenize(sentence.lower())\n",
        "  sentence_score=0\n",
        "  for word in sentence_words:\n",
        "    if stemmer.stem(word) in top_words:\n",
        "      sentence_score += 1\n",
        "  summary.append((sentence,sentence_score))\n",
        "#Print the summary\n",
        "for sentence in nlargest(3,summary,key=lambda x: x[1]):\n",
        "  print(sentence[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyk4OXqjpyYZ",
        "outputId": "63ffe7ca-dc3a-40e4-fcd3-f47f87c408e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "The quick brown fox jumps over the lazy dog.\n",
            "The fox is running fast to catch its prey.\n",
            "The prey escapes and the fox continues to run.\n"
          ]
        }
      ]
    }
  ]
}